{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CUDA_Parrallel_Reduction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aditya-malte/Simple-LP1-Codes/blob/master/HPC1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG83fqWNsOz2",
        "colab_type": "text"
      },
      "source": [
        "##Select Runtime as GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "od_h5OFyWUOu",
        "colab_type": "code",
        "outputId": "0073e4d9-2297-413e-c5ab-3e2ec38703b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
            "Cuda compilation tools, release 10.0, V10.0.130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvvfu1PvWX8p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "code = \"\"\"\n",
        "#include <iostream>\n",
        "using namespace std;\n",
        "\n",
        "__global__ void sum(int* input) //this code runs on GPU\n",
        "{\n",
        "    const int tid = threadIdx.x;    //get thread id\n",
        "    int step_size = 1; //set initial step_size\n",
        "    int number_of_threads = blockDim.x; //get number of threads in block\n",
        "    while (number_of_threads > 0)\n",
        "    {\n",
        "        if (tid < number_of_threads)\n",
        "        {\n",
        "            const int fst = tid * step_size * 2;\n",
        "            const int snd = fst + step_size;\n",
        "            printf(\"tid=%d fst=%d snd=%d|sum=%d\\\\n\", threadIdx.x, input[fst], input[snd], input[fst]+input[snd]);\n",
        "            input[fst] += input[snd];\n",
        "        }\n",
        "        step_size <<= 1;  //shift left operation doubles step size\n",
        "        number_of_threads >>= 1;  //half the number of threads\n",
        "    }\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    const int count = 8;\n",
        "    const int size = count * sizeof(int);\n",
        "    int h[] = {13, 27, 15, 14, 33, 2, 24, 6};   //input stored at host\n",
        "    for(int i = 0; i<count; i++)\n",
        "    {\n",
        "      printf(\"%d,\", h[i]);\n",
        "    }\n",
        "    printf(\"\\\\n\");\n",
        "    int* d;       //pointer stored at device\n",
        "\n",
        "    cudaMalloc(&d, size); //allocate d with size of array\n",
        "    cudaMemcpy(d, h, size, cudaMemcpyHostToDevice); //copy data from host to device\n",
        "\n",
        "    sum <<<1, count / 2 >>>(d); //call function with one block and count/2 threads\n",
        "\n",
        "    int result;\n",
        "    cudaMemcpy(&result, d, sizeof(int), cudaMemcpyDeviceToHost);  //copy result to host\n",
        "\n",
        "    printf(\"\\\\n Sum is: %d\", result);\n",
        "    //free values\n",
        "    cudaFree(d);\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-sv3mWlWvd-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_file = open(\"code.cu\", \"w\")\n",
        "text_file.write(code)\n",
        "text_file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWGAyAbzXGQj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvcc code.cu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dozKqOBWXSPI",
        "colab_type": "code",
        "outputId": "0f4881f6-bccb-44b6-aaef-6b7a3d402cb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "!./a.out"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13,27,15,14,33,2,24,6,\n",
            "tid=0 fst=13 snd=27|sum=40\n",
            "tid=1 fst=15 snd=14|sum=29\n",
            "tid=2 fst=33 snd=2|sum=35\n",
            "tid=3 fst=24 snd=6|sum=30\n",
            "tid=0 fst=40 snd=29|sum=69\n",
            "tid=1 fst=35 snd=30|sum=65\n",
            "tid=0 fst=69 snd=65|sum=134\n",
            "\n",
            " Sum is: 134"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y736gcd3bbht",
        "colab_type": "code",
        "outputId": "bb9ff89a-810f-4eae-d705-6cc4f107c62c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "!nvprof ./a.out"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13,27,15,14,33,2,24,6,\n",
            "==425== NVPROF is profiling process 425, command: ./a.out\n",
            "tid=0 fst=13 snd=27|sum=40\n",
            "tid=1 fst=15 snd=14|sum=29\n",
            "tid=2 fst=33 snd=2|sum=35\n",
            "tid=3 fst=24 snd=6|sum=30\n",
            "tid=0 fst=40 snd=29|sum=69\n",
            "tid=1 fst=35 snd=30|sum=65\n",
            "tid=0 fst=69 snd=65|sum=134\n",
            "\n",
            " Sum is: 134==425== Profiling application: ./a.out\n",
            "==425== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   98.90%  467.30us         1  467.30us  467.30us  467.30us  sum(int*)\n",
            "                    0.62%  2.9440us         1  2.9440us  2.9440us  2.9440us  [CUDA memcpy DtoH]\n",
            "                    0.48%  2.2720us         1  2.2720us  2.2720us  2.2720us  [CUDA memcpy HtoD]\n",
            "      API calls:   98.45%  144.74ms         1  144.74ms  144.74ms  144.74ms  cudaMalloc\n",
            "                    0.48%  705.32us         1  705.32us  705.32us  705.32us  cudaLaunchKernel\n",
            "                    0.38%  552.99us         1  552.99us  552.99us  552.99us  cuDeviceTotalMem\n",
            "                    0.37%  544.95us         2  272.48us  26.507us  518.45us  cudaMemcpy\n",
            "                    0.22%  318.26us        96  3.3150us     142ns  138.06us  cuDeviceGetAttribute\n",
            "                    0.08%  113.50us         1  113.50us  113.50us  113.50us  cudaFree\n",
            "                    0.02%  34.441us         1  34.441us  34.441us  34.441us  cuDeviceGetName\n",
            "                    0.00%  3.7080us         1  3.7080us  3.7080us  3.7080us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.2380us         3     746ns     211ns  1.2940us  cuDeviceGetCount\n",
            "                    0.00%  1.2830us         2     641ns     269ns  1.0140us  cuDeviceGet\n",
            "                    0.00%     342ns         1     342ns     342ns     342ns  cuDeviceGetUuid\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}